{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fa4c76",
   "metadata": {},
   "source": [
    "# Stock Price Projection Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0eb92",
   "metadata": {},
   "source": [
    "## Seting Up enviroment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d427cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install required libraries (run once)\n",
    "!pip install -q yfinance pandas numpy matplotlib seaborn scikit-learn statsmodels tensorflow datasets ta\n",
    "\n",
    "# Notes:\n",
    "# - 'ta' is a lightweight technical analysis library (alternative to TA-Lib).\n",
    "# - 'datasets' is the Hugging Face library to load their datasets.\n",
    "# - Use '-q' to suppress verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3a28c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duvan/Documents/StockPredictionModel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-17 13:35:51.918320: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-17 13:35:52.188029: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-17 13:35:53.884286: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-17 13:35:54,227 - INFO - Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import libraries and set up logging\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set up logging to file and console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"stock_project.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directories for data and models\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "logger.info(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51421fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-17 13:35:54,235 - INFO - Configuration set for NVDA from 2010-01-01 to 2025-12-31\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration parameters\n",
    "TICKER = \"NVDA\"\n",
    "START_DATE = \"2010-01-01\"\n",
    "END_DATE = \"2025-12-31\"\n",
    "VIX_TICKER = \"^VIX\"\n",
    "WINDOW_SIZE = 60          # for sequence creation\n",
    "TEST_SIZE = 0.2            # proportion of data for testing (if not using date split)\n",
    "VAL_SIZE = 0.1             # proportion of training data for validation\n",
    "\n",
    "# Paths\n",
    "RAW_DATA_PATH = \"data/raw\"\n",
    "PROCESSED_DATA_PATH = \"data/processed\"\n",
    "\n",
    "logger.info(f\"Configuration set for {TICKER} from {START_DATE} to {END_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc87425b",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb2dbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-17 13:35:54,244 - INFO - Downloading NVDA data from Yahoo Finance...\n",
      "2026-02-17 13:35:56,001 - INFO - Downloaded 4023 rows from Yahoo Finance.\n",
      "2026-02-17 13:35:56,002 - INFO - Downloading VIX data...\n",
      "2026-02-17 13:35:56,742 - ERROR - Failed to download VIX: 'Adj Close'\n",
      "2026-02-17 13:35:56,766 - INFO - Raw Yahoo data saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Download NVIDIA data from Yahoo Finance\n",
    "logger.info(f\"Downloading {TICKER} data from Yahoo Finance...\")\n",
    "try:\n",
    "    nvda_yf = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=False)\n",
    "    logger.info(f\"Downloaded {len(nvda_yf)} rows from Yahoo Finance.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to download {TICKER}: {e}\")\n",
    "    nvda_yf = pd.DataFrame()\n",
    "\n",
    "# Download VIX (volatility index) as an additional feature\n",
    "logger.info(\"Downloading VIX data...\")\n",
    "try:\n",
    "    vix = yf.download(VIX_TICKER, start=START_DATE, end=END_DATE, progress=False)['Adj Close']\n",
    "    vix.name = 'VIX'\n",
    "    logger.info(f\"Downloaded {len(vix)} rows of VIX.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to download VIX: {e}\")\n",
    "    vix = pd.Series(dtype=float)\n",
    "\n",
    "# Save raw Yahoo data\n",
    "if not nvda_yf.empty:\n",
    "    nvda_yf.to_csv(os.path.join(RAW_DATA_PATH, f\"{TICKER}_yf.csv\"))\n",
    "    vix.to_csv(os.path.join(RAW_DATA_PATH, \"VIX.csv\"))\n",
    "    logger.info(\"Raw Yahoo data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bcb097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-17 13:35:56,774 - INFO - Loading MTBench stock dataset from Hugging Face...\n",
      "2026-02-17 13:35:57,312 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/afeng/MTBench_finance_stock/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-17 13:35:57,415 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/GGLabYale/MTBench_finance_stock/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-17 13:35:57,510 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/datasets/GGLabYale/MTBench_finance_stock/0c1a656a9611846e76c35a96ec85404829763022/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-02-17 13:35:57,624 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/afeng/MTBench_finance_stock/resolve/0c1a656a9611846e76c35a96ec85404829763022/MTBench_finance_stock.py \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-17 13:35:57,729 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/GGLabYale/MTBench_finance_stock/resolve/0c1a656a9611846e76c35a96ec85404829763022/MTBench_finance_stock.py \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-17 13:35:58,247 - INFO - HTTP Request: HEAD https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/afeng/MTBench_finance_stock/afeng/MTBench_finance_stock.py \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-17 13:35:58,250 - WARNING - Repo card metadata block was not found. Setting CardData to empty.\n",
      "2026-02-17 13:35:58,432 - INFO - HTTP Request: GET https://huggingface.co/api/datasets/afeng/MTBench_finance_stock/revision/0c1a656a9611846e76c35a96ec85404829763022 \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-17 13:35:58,546 - INFO - HTTP Request: GET https://huggingface.co/api/datasets/GGLabYale/MTBench_finance_stock/revision/0c1a656a9611846e76c35a96ec85404829763022 \"HTTP/1.1 200 OK\"\n",
      "2026-02-17 13:35:58,657 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/afeng/MTBench_finance_stock/resolve/0c1a656a9611846e76c35a96ec85404829763022/.huggingface.yaml \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-17 13:35:58,783 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/GGLabYale/MTBench_finance_stock/resolve/0c1a656a9611846e76c35a96ec85404829763022/.huggingface.yaml \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-17 13:35:59,251 - INFO - HTTP Request: GET https://datasets-server.huggingface.co/info?dataset=afeng/MTBench_finance_stock \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-17 13:35:59,456 - INFO - HTTP Request: GET https://huggingface.co/api/datasets/afeng/MTBench_finance_stock/tree/0c1a656a9611846e76c35a96ec85404829763022/data?recursive=true&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-17 13:35:59,574 - INFO - HTTP Request: GET https://huggingface.co/api/datasets/GGLabYale/MTBench_finance_stock/tree/0c1a656a9611846e76c35a96ec85404829763022/data?recursive=true&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-17 13:35:59,763 - INFO - HTTP Request: GET https://huggingface.co/api/datasets/afeng/MTBench_finance_stock/tree/0c1a656a9611846e76c35a96ec85404829763022?recursive=false&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-17 13:35:59,971 - INFO - HTTP Request: GET https://huggingface.co/api/datasets/GGLabYale/MTBench_finance_stock/tree/0c1a656a9611846e76c35a96ec85404829763022?recursive=false&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-02-17 13:36:00,378 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/afeng/MTBench_finance_stock/resolve/0c1a656a9611846e76c35a96ec85404829763022/dataset_infos.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-17 13:36:00,628 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/GGLabYale/MTBench_finance_stock/resolve/0c1a656a9611846e76c35a96ec85404829763022/dataset_infos.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-17 13:36:00,739 - INFO - MTBench dataset loaded. Structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'vwap', 'transactions', 'otc', 'real_timestamp'],\n",
      "        num_rows: 2813\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load MTBench dataset (high-frequency 5-min data)\n",
    "logger.info(\"Loading MTBench stock dataset from Hugging Face...\")\n",
    "# This will download the dataset (approx 2-3 GB). It may take a while.\n",
    "try:\n",
    "    mtbench_dataset = load_dataset(\"afeng/MTBench_finance_stock\")\n",
    "    logger.info(f\"MTBench dataset loaded. Structure: {mtbench_dataset}\")\n",
    "    # Save dataset info for later reference\n",
    "    with open(os.path.join(RAW_DATA_PATH, \"mtbench_info.txt\"), \"w\") as f:\n",
    "        f.write(str(mtbench_dataset))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load MTBench: {e}\")\n",
    "    mtbench_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d54b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Extract NVIDIA data from MTBench (if available)\n",
    "if mtbench_dataset is not None:\n",
    "    # The MTBench dataset structure: we need to filter for NVDA.\n",
    "    # Usually, it's a DatasetDict with 'train' split containing all stocks.\n",
    "    # We'll assume the 'train' split has a 'ticker' column.\n",
    "    try:\n",
    "        # Convert to pandas for easier filtering (be mindful of memory)\n",
    "        df_mtbench = mtbench_dataset['train'].to_pandas()\n",
    "        nvda_mtbench = df_mtbench[df_mtbench['ticker'] == 'NVDA'].copy()\n",
    "        logger.info(f\"Extracted {len(nvda_mtbench)} rows for NVDA from MTBench.\")\n",
    "        # Save to CSV\n",
    "        nvda_mtbench.to_csv(os.path.join(RAW_DATA_PATH, \"NVDA_MTBench.csv\"), index=False)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract NVDA from MTBench: {e}\")\n",
    "        nvda_mtbench = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load FNSPID dataset (news + prices)\n",
    "logger.info(\"Loading FNSPID dataset (this is large, may take time)...\")\n",
    "try:\n",
    "    # The FNSPID dataset is available at https://huggingface.co/datasets/Zihan1004/FNSPID\n",
    "    # It contains both price and news data. We'll load the full dataset.\n",
    "    fnspid_dataset = load_dataset(\"Zihan1004/FNSPID\")\n",
    "    logger.info(f\"FNSPID loaded. Structure: {fnspid_dataset}\")\n",
    "    with open(os.path.join(RAW_DATA_PATH, \"fnspid_info.txt\"), \"w\") as f:\n",
    "        f.write(str(fnspid_dataset))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load FNSPID: {e}\")\n",
    "    fnspid_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: (Optional) Load NVIDIA-specific fundamental dataset\n",
    "logger.info(\"Loading NVIDIA fundamental dataset from Hugging Face...\")\n",
    "try:\n",
    "    nvda_fundamental = load_dataset(\"imdad19/Nvidia_data\")\n",
    "    logger.info(f\"Fundamental dataset loaded: {nvda_fundamental}\")\n",
    "    # Convert to DataFrame and save\n",
    "    df_fund = nvda_fundamental['train'].to_pandas()\n",
    "    df_fund.to_csv(os.path.join(RAW_DATA_PATH, \"NVDA_fundamental.csv\"), index=False)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load fundamental dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f44033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Quick look at the downloaded data\n",
    "logger.info(\"Sample of Yahoo Finance data:\")\n",
    "display(nvda_yf.head())\n",
    "\n",
    "if not nvda_mtbench.empty:\n",
    "    logger.info(\"Sample of MTBench data for NVDA:\")\n",
    "    display(nvda_mtbench.head())\n",
    "\n",
    "logger.info(\"Phase 1 complete. All raw data saved in 'data/raw/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336ab60",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Import additional libraries for EDA (if not already imported)\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "logger.info(\"Starting Phase 2: Exploratory Data Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Load the Yahoo Finance data (saved earlier)\n",
    "nvda_yf = pd.read_csv(os.path.join(RAW_DATA_PATH, f\"{TICKER}_yf.csv\"), index_col=0, parse_dates=True)\n",
    "vix = pd.read_csv(os.path.join(RAW_DATA_PATH, \"VIX.csv\"), index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "# Ensure datetime index\n",
    "nvda_yf.index = pd.to_datetime(nvda_yf.index)\n",
    "vix.index = pd.to_datetime(vix.index)\n",
    "\n",
    "logger.info(f\"Data loaded: {nvda_yf.shape[0]} rows, {nvda_yf.shape[1]} columns\")\n",
    "logger.info(f\"Date range: {nvda_yf.index.min()} to {nvda_yf.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Basic info and missing values\n",
    "print(\"Data types:\")\n",
    "print(nvda_yf.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(nvda_yf.isnull().sum())\n",
    "\n",
    "# Plot missing values (if any)\n",
    "if nvda_yf.isnull().sum().sum() > 0:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.heatmap(nvda_yf.isnull(), cbar=False, yticklabels=False)\n",
    "    plt.title(\"Missing Values Heatmap\")\n",
    "    plt.show()\n",
    "else:\n",
    "    logger.info(\"No missing values in Yahoo Finance data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98230561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Summary statistics\n",
    "print(\"Summary statistics (full period):\")\n",
    "display(nvda_yf.describe())\n",
    "\n",
    "# Split into pre-2020 and post-2020 to see changes\n",
    "pre_2020 = nvda_yf[nvda_yf.index < '2020-01-01']\n",
    "post_2020 = nvda_yf[nvda_yf.index >= '2020-01-01']\n",
    "\n",
    "print(\"\\nPre-2020 statistics:\")\n",
    "display(pre_2020.describe())\n",
    "print(\"\\nPost-2020 statistics:\")\n",
    "display(post_2020.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ff291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Plot closing price with volume\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Price plot\n",
    "axes[0].plot(nvda_yf.index, nvda_yf['Close'], label='Close Price', color='blue')\n",
    "axes[0].set_title(f'{TICKER} Closing Price Over Time', fontsize=16)\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Volume plot\n",
    "axes[1].bar(nvda_yf.index, nvda_yf['Volume'], color='gray', alpha=0.6, width=2)\n",
    "axes[1].set_title('Trading Volume', fontsize=16)\n",
    "axes[1].set_ylabel('Volume')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"price_volume.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Calculate daily returns and visualize\n",
    "nvda_yf['Return'] = nvda_yf['Close'].pct_change() * 100  # percentage returns\n",
    "nvda_yf['Log_Return'] = np.log(nvda_yf['Close'] / nvda_yf['Close'].shift(1))\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "# Returns over time\n",
    "axes[0].plot(nvda_yf.index, nvda_yf['Return'], color='green', alpha=0.7, linewidth=0.8)\n",
    "axes[0].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[0].set_title('Daily Percentage Returns', fontsize=14)\n",
    "axes[0].set_ylabel('Return (%)')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Histogram of returns\n",
    "axes[1].hist(nvda_yf['Return'].dropna(), bins=100, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Distribution of Daily Returns', fontsize=14)\n",
    "axes[1].set_xlabel('Return (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Q-Q plot for normality\n",
    "from scipy import stats\n",
    "stats.probplot(nvda_yf['Return'].dropna(), dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot of Returns', fontsize=14)\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"returns_analysis.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Summary stats for returns\n",
    "print(\"Return statistics:\")\n",
    "print(nvda_yf['Return'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d33a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Check stationarity of closing price and returns\n",
    "def check_stationarity(series, title):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'ADF Statistic for {title}: {result[0]:.6f}')\n",
    "    print(f'p-value: {result[1]:.6f}')\n",
    "    print('Critical values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"=> Series is stationary (reject H0)\\n\")\n",
    "    else:\n",
    "        print(\"=> Series is non-stationary (fail to reject H0)\\n\")\n",
    "\n",
    "check_stationarity(nvda_yf['Close'], 'Close Price')\n",
    "check_stationarity(nvda_yf['Return'], 'Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd13ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Autocorrelation and Partial Autocorrelation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\n",
    "# ACF and PACF of closing price (non-stationary, just for illustration)\n",
    "plot_acf(nvda_yf['Close'].dropna(), lags=40, ax=axes[0,0])\n",
    "axes[0,0].set_title('ACF of Close Price')\n",
    "plot_pacf(nvda_yf['Close'].dropna(), lags=40, ax=axes[0,1])\n",
    "axes[0,1].set_title('PACF of Close Price')\n",
    "\n",
    "# ACF and PACF of returns (stationary)\n",
    "plot_acf(nvda_yf['Return'].dropna(), lags=40, ax=axes[1,0])\n",
    "axes[1,0].set_title('ACF of Returns')\n",
    "plot_pacf(nvda_yf['Return'].dropna(), lags=40, ax=axes[1,1])\n",
    "axes[1,1].set_title('PACF of Returns')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"acf_pacf.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Technical Indicators using 'ta' library\n",
    "import ta\n",
    "\n",
    "# Add common indicators\n",
    "nvda_yf['SMA_10'] = ta.trend.sma_indicator(nvda_yf['Close'], window=10)\n",
    "nvda_yf['SMA_30'] = ta.trend.sma_indicator(nvda_yf['Close'], window=30)\n",
    "nvda_yf['EMA_12'] = ta.trend.ema_indicator(nvda_yf['Close'], window=12)\n",
    "nvda_yf['EMA_26'] = ta.trend.ema_indicator(nvda_yf['Close'], window=26)\n",
    "nvda_yf['RSI'] = ta.momentum.rsi(nvda_yf['Close'], window=14)\n",
    "macd = ta.trend.MACD(nvda_yf['Close'])\n",
    "nvda_yf['MACD'] = macd.macd()\n",
    "nvda_yf['MACD_signal'] = macd.macd_signal()\n",
    "nvda_yf['MACD_diff'] = macd.macd_diff()\n",
    "\n",
    "# Bollinger Bands\n",
    "bb = ta.volatility.BollingerBands(nvda_yf['Close'], window=20, window_dev=2)\n",
    "nvda_yf['BB_high'] = bb.bollinger_hband()\n",
    "nvda_yf['BB_low'] = bb.bollinger_lband()\n",
    "nvda_yf['BB_width'] = bb.bollinger_wband()\n",
    "\n",
    "# Volume-based: On-Balance Volume (OBV)\n",
    "nvda_yf['OBV'] = ta.volume.on_balance_volume(nvda_yf['Close'], nvda_yf['Volume'])\n",
    "\n",
    "logger.info(\"Technical indicators added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ba36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Visualize some indicators\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 14), sharex=True)\n",
    "\n",
    "# Price with SMAs\n",
    "axes[0].plot(nvda_yf.index, nvda_yf['Close'], label='Close', alpha=0.6, linewidth=1)\n",
    "axes[0].plot(nvda_yf.index, nvda_yf['SMA_10'], label='SMA 10', linestyle='--')\n",
    "axes[0].plot(nvda_yf.index, nvda_yf['SMA_30'], label='SMA 30', linestyle='--')\n",
    "axes[0].set_title('Price with Moving Averages')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# RSI\n",
    "axes[1].plot(nvda_yf.index, nvda_yf['RSI'], color='purple')\n",
    "axes[1].axhline(y=70, color='red', linestyle='--', label='Overbought (70)')\n",
    "axes[1].axhline(y=30, color='green', linestyle='--', label='Oversold (30)')\n",
    "axes[1].set_title('Relative Strength Index (RSI)')\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# MACD\n",
    "axes[2].plot(nvda_yf.index, nvda_yf['MACD'], label='MACD', color='blue')\n",
    "axes[2].plot(nvda_yf.index, nvda_yf['MACD_signal'], label='Signal', color='orange')\n",
    "axes[2].bar(nvda_yf.index, nvda_yf['MACD_diff'], label='Histogram', color='gray', alpha=0.5)\n",
    "axes[2].set_title('MACD')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "# Bollinger Band Width (volatility)\n",
    "axes[3].fill_between(nvda_yf.index, nvda_yf['BB_low'], nvda_yf['BB_high'], alpha=0.3, color='gray', label='Bollinger Bands')\n",
    "axes[3].plot(nvda_yf.index, nvda_yf['Close'], label='Close', color='black', linewidth=1)\n",
    "axes[3].set_title('Price with Bollinger Bands')\n",
    "axes[3].legend()\n",
    "axes[3].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"technical_indicators.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Merge VIX and check correlation\n",
    "nvda_yf = nvda_yf.join(vix, how='left')\n",
    "# Forward fill VIX (since VIX is not available on all days? Actually VIX has trading days, so align)\n",
    "nvda_yf['VIX'] = nvda_yf['VIX'].fillna(method='ffill')\n",
    "\n",
    "# Compute correlation matrix for selected features\n",
    "features_for_corr = ['Close', 'Volume', 'Return', 'RSI', 'MACD', 'VIX', 'OBV', 'BB_width']\n",
    "corr_matrix = nvda_yf[features_for_corr].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', square=True)\n",
    "plt.title('Correlation Matrix of Selected Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"correlation_matrix.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Seasonality analysis – average returns by day of week, month\n",
    "nvda_yf['DayOfWeek'] = nvda_yf.index.dayofweek  # Monday=0, Sunday=6\n",
    "nvda_yf['Month'] = nvda_yf.index.month\n",
    "nvda_yf['Year'] = nvda_yf.index.year\n",
    "\n",
    "# Average return by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "day_avg = nvda_yf.groupby('DayOfWeek')['Return'].mean().rename(dict(enumerate(day_order)))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "day_avg.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Average Daily Return by Weekday')\n",
    "axes[0].set_ylabel('Avg Return (%)')\n",
    "axes[0].set_xticklabels(day_order, rotation=45)\n",
    "\n",
    "# Average return by month\n",
    "month_avg = nvda_yf.groupby('Month')['Return'].mean()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "month_avg.index = month_names\n",
    "month_avg.plot(kind='bar', ax=axes[1], color='lightgreen')\n",
    "axes[1].set_title('Average Daily Return by Month')\n",
    "axes[1].set_ylabel('Avg Return (%)')\n",
    "axes[1].set_xticklabels(month_names, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"seasonality.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690889cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Check for outliers in returns\n",
    "# Using IQR method\n",
    "Q1 = nvda_yf['Return'].quantile(0.25)\n",
    "Q3 = nvda_yf['Return'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = nvda_yf[(nvda_yf['Return'] < lower_bound) | (nvda_yf['Return'] > upper_bound)]\n",
    "print(f\"Number of outlier days (returns): {len(outliers)} ({len(outliers)/len(nvda_yf)*100:.2f}%)\")\n",
    "print(\"Outlier dates and returns:\")\n",
    "print(outliers[['Return']].sort_values('Return'))\n",
    "\n",
    "# Plot with outliers highlighted\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(nvda_yf.index, nvda_yf['Return'], color='blue', alpha=0.5)\n",
    "plt.scatter(outliers.index, outliers['Return'], color='red', s=20, label='Outliers')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.title('Daily Returns with Outliers Highlighted')\n",
    "plt.ylabel('Return (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"outliers.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ab8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Quick look at high-frequency MTBench data (if available)\n",
    "if not nvda_mtbench.empty:\n",
    "    logger.info(\"Exploring MTBench high-frequency data for NVDA...\")\n",
    "    # Convert timestamp column to datetime\n",
    "    nvda_mtbench['timestamp'] = pd.to_datetime(nvda_mtbench['timestamp'])\n",
    "    nvda_mtbench.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # Plot one day of 5-min data as example\n",
    "    sample_day = '2023-06-01'\n",
    "    if sample_day in nvda_mtbench.index:\n",
    "        day_data = nvda_mtbench.loc[sample_day]\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        plt.plot(day_data.index, day_data['close'], marker='o', linestyle='-', markersize=3)\n",
    "        plt.title(f'NVDA 5-minute Price on {sample_day}')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Price ($)')\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"mtbench_sample.png\"))\n",
    "        plt.show()\n",
    "    else:\n",
    "        logger.info(f\"Sample day {sample_day} not in MTBench data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Save processed daily data with features for modeling\n",
    "# Drop rows with NaN from indicator calculation (first few rows)\n",
    "nvda_processed = nvda_yf.dropna().copy()\n",
    "nvda_processed.to_csv(os.path.join(PROCESSED_DATA_PATH, f\"{TICKER}_processed.csv\"))\n",
    "logger.info(f\"Processed daily data saved: {nvda_processed.shape[0]} rows, {nvda_processed.shape[1]} columns\")\n",
    "\n",
    "# Also save the feature list\n",
    "with open(os.path.join(PROCESSED_DATA_PATH, \"feature_list.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(nvda_processed.columns.tolist()))\n",
    "\n",
    "logger.info(\"Phase 2 complete. EDA results and processed data saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9a2f1",
   "metadata": {},
   "source": [
    "## Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Load the processed daily data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import joblib\n",
    "\n",
    "PROCESSED_DATA_PATH = \"data/processed\"\n",
    "TICKER = \"NVDA\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, f\"{TICKER}_processed.csv\"), index_col=0, parse_dates=True)\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5407f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27: Define target variables\n",
    "# For regression: next day's closing price\n",
    "df['Target_Close'] = df['Close'].shift(-1)\n",
    "\n",
    "# For classification: next day's direction (1 if up, 0 if down)\n",
    "df['Target_Dir'] = (df['Target_Close'] > df['Close']).astype(int)\n",
    "\n",
    "# Drop rows with NaN targets (last row)\n",
    "df.dropna(subset=['Target_Close', 'Target_Dir'], inplace=True)\n",
    "\n",
    "print(f\"After target creation: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd979bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28: Feature engineering – additional predictors\n",
    "# Rolling volatility (standard deviation of returns over 20 days)\n",
    "df['Volatility_20'] = df['Return'].rolling(20).std()\n",
    "\n",
    "# Price momentum: (Close / Close shifted n) - 1\n",
    "for lag in [5, 10, 21]:\n",
    "    df[f'Momentum_{lag}'] = df['Close'] / df['Close'].shift(lag) - 1\n",
    "\n",
    "# Volume change\n",
    "df['Volume_change'] = df['Volume'].pct_change()\n",
    "\n",
    "# RSI change\n",
    "df['RSI_change'] = df['RSI'].diff()\n",
    "\n",
    "# MACD histogram change\n",
    "df['MACD_hist_change'] = df['MACD_diff'].diff()\n",
    "\n",
    "# Day of week (already created, but ensure it's cyclical encoded)\n",
    "df['Day_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "df['Day_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "\n",
    "# Month cyclical encoding\n",
    "df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "# Drop original categorical columns (optional)\n",
    "df.drop(['DayOfWeek', 'Month', 'Year'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Drop rows with NaN from new features\n",
    "df.dropna(inplace=True)\n",
    "print(f\"After feature engineering: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29: Select features for modeling\n",
    "# We'll choose a set of features that are likely useful.\n",
    "# Exclude price columns that would cause look-ahead if used carelessly.\n",
    "feature_columns = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "    'SMA_10', 'SMA_30', 'EMA_12', 'EMA_26',\n",
    "    'RSI', 'MACD', 'MACD_signal', 'MACD_diff',\n",
    "    'BB_high', 'BB_low', 'BB_width', 'OBV',\n",
    "    'VIX', 'Return', 'Log_Return',\n",
    "    'Volatility_20', 'Momentum_5', 'Momentum_10', 'Momentum_21',\n",
    "    'Volume_change', 'RSI_change', 'MACD_hist_change',\n",
    "    'Day_sin', 'Day_cos', 'Month_sin', 'Month_cos'\n",
    "]\n",
    "\n",
    "# Ensure all selected columns exist in df\n",
    "feature_columns = [col for col in feature_columns if col in df.columns]\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(\"Features:\", feature_columns)\n",
    "\n",
    "# Separate features and targets\n",
    "X = df[feature_columns]\n",
    "y_reg = df['Target_Close']\n",
    "y_clf = df['Target_Dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30: Chronological split\n",
    "# Define split dates based on typical train/val/test proportions (e.g., 70/15/15)\n",
    "total_len = len(df)\n",
    "train_end = int(0.7 * total_len)\n",
    "val_end = int(0.85 * total_len)\n",
    "\n",
    "# Get the corresponding dates\n",
    "train_dates = df.index[:train_end]\n",
    "val_dates = df.index[train_end:val_end]\n",
    "test_dates = df.index[val_end:]\n",
    "\n",
    "print(f\"Train: {train_dates[0]} to {train_dates[-1]} ({len(train_dates)} days)\")\n",
    "print(f\"Validation: {val_dates[0]} to {val_dates[-1]} ({len(val_dates)} days)\")\n",
    "print(f\"Test: {test_dates[0]} to {test_dates[-1]} ({len(test_dates)} days)\")\n",
    "\n",
    "# Split indices\n",
    "X_train = X.loc[train_dates]\n",
    "y_reg_train = y_reg.loc[train_dates]\n",
    "y_clf_train = y_clf.loc[train_dates]\n",
    "\n",
    "X_val = X.loc[val_dates]\n",
    "y_reg_val = y_reg.loc[val_dates]\n",
    "y_clf_val = y_clf.loc[val_dates]\n",
    "\n",
    "X_test = X.loc[test_dates]\n",
    "y_reg_test = y_reg.loc[test_dates]\n",
    "y_clf_test = y_clf.loc[test_dates]\n",
    "\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_reg_train: {y_reg_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_reg_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_reg_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f647a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 31: Scale the features\n",
    "# Fit scaler on training data only, then transform all sets\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use (e.g., inverse transform predictions)\n",
    "scaler_path = os.path.join(PROCESSED_DATA_PATH, \"scaler.pkl\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "# Convert back to DataFrames with column names (optional)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8079204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 32: Create sequences for RNN/LSTM/GRU/Transformer models\n",
    "def create_sequences(X, y, window_size):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window_size, len(X)):\n",
    "        X_seq.append(X[i-window_size:i])\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "WINDOW_SIZE = 60  # use 60 days of history to predict next day\n",
    "\n",
    "# Create sequences for each set\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled.values, y_reg_train.values, WINDOW_SIZE)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled.values, y_reg_val.values, WINDOW_SIZE)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled.values, y_reg_test.values, WINDOW_SIZE)\n",
    "\n",
    "print(f\"Training sequences: {X_train_seq.shape}, targets: {y_train_seq.shape}\")\n",
    "print(f\"Validation sequences: {X_val_seq.shape}, targets: {y_val_seq.shape}\")\n",
    "print(f\"Test sequences: {X_test_seq.shape}, targets: {y_test_seq.shape}\")\n",
    "\n",
    "# For classification, we could also create sequences using y_clf.\n",
    "# We'll handle that separately when building classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 33: Save the prepared sequences for modeling\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, \"X_train_seq.npy\"), X_train_seq)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, \"y_train_seq.npy\"), y_train_seq)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, \"X_val_seq.npy\"), X_val_seq)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, \"y_val_seq.npy\"), y_val_seq)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, \"X_test_seq.npy\"), X_test_seq)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, \"y_test_seq.npy\"), y_test_seq)\n",
    "\n",
    "# Also save the dates corresponding to each sequence element for reference\n",
    "# The target date for each sequence is the date of y (i.e., the prediction day)\n",
    "train_seq_dates = X_train.index[WINDOW_SIZE:]\n",
    "val_seq_dates = X_val.index[WINDOW_SIZE:]\n",
    "test_seq_dates = X_test.index[WINDOW_SIZE:]\n",
    "\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, \"train_seq_dates.npy\"), train_seq_dates)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, \"val_seq_dates.npy\"), val_seq_dates)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, \"test_seq_dates.npy\"), test_seq_dates)\n",
    "\n",
    "print(\"Sequence data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 34: Quick sanity check – plot a few sequences\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    # Plot the first feature (Close price) from a few sequences\n",
    "    idx = i * 100  # spacing\n",
    "    if idx < len(X_train_seq):\n",
    "        plt.plot(X_train_seq[idx, :, feature_columns.index('Close')])\n",
    "        plt.title(f'Train Seq {idx} - Close')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"sequence_samples.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e897d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 35: (Optional) Handle high-frequency MTBench data for future use\n",
    "# If MTBench data was successfully loaded and processed, we might want to downsample it to daily\n",
    "# or use it directly in a hybrid model. This is a placeholder for future expansion.\n",
    "if os.path.exists(os.path.join(RAW_DATA_PATH, \"NVDA_MTBench.csv\")):\n",
    "    print(\"MTBench data available. Consider integrating it later.\")\n",
    "    # For now, we'll just note it.\n",
    "else:\n",
    "    print(\"MTBench data not found – continuing with daily data only.\")\n",
    "\n",
    "logger.info(\"Phase 3 complete. Data ready for modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e410bb",
   "metadata": {},
   "source": [
    "## Baseline ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 36: Import additional libraries for ARIMA\n",
    "import pmdarima as pm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logger.info(\"Starting Phase 4: ARIMA Baseline Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b10faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 37: Load the processed data (if starting fresh)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "PROCESSED_DATA_PATH = \"data/processed\"\n",
    "TICKER = \"NVDA\"\n",
    "\n",
    "# Load the daily data with targets\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, f\"{TICKER}_processed.csv\"), index_col=0, parse_dates=True)\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "# Extract the closing price series\n",
    "close_series = df['Close'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18734ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 38: Use the same chronological split as Phase 3\n",
    "# We need the split indices for walk-forward validation\n",
    "# Load the saved split dates (or recompute from indices)\n",
    "train_dates = df.index[:int(0.7*len(df))]\n",
    "val_dates = df.index[int(0.7*len(df)):int(0.85*len(df))]\n",
    "test_dates = df.index[int(0.85*len(df)):]\n",
    "\n",
    "# Split the series accordingly\n",
    "train_close = close_series.loc[train_dates]\n",
    "val_close = close_series.loc[val_dates]\n",
    "test_close = close_series.loc[test_dates]\n",
    "\n",
    "print(f\"Train: {train_close.index[0]} to {train_close.index[-1]} ({len(train_close)} days)\")\n",
    "print(f\"Validation: {val_close.index[0]} to {val_close.index[-1]} ({len(val_close)} days)\")\n",
    "print(f\"Test: {test_close.index[0]} to {test_close.index[-1]} ({len(test_close)} days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b72293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 39: Check stationarity again (for completeness)\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series, title):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'{title} ADF Statistic: {result[0]:.6f}')\n",
    "    print(f'p-value: {result[1]:.6f}')\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"=> Series is stationary\\n\")\n",
    "    else:\n",
    "        print(\"=> Series is non-stationary\\n\")\n",
    "\n",
    "adf_test(train_close, 'Train Close Price')\n",
    "adf_test(train_close.diff().dropna(), 'Train Close Price (1st diff)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45386999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 40: Automatic ARIMA order selection using auto_arima\n",
    "# We'll use pmdarima which handles seasonal patterns automatically (though we don't expect strong seasonality).\n",
    "# Set a maximum order to avoid excessive computation time.\n",
    "\n",
    "logger.info(\"Running auto_arima to find best (p,d,q) on training data...\")\n",
    "auto_model = pm.auto_arima(\n",
    "    train_close,\n",
    "    start_p=0, max_p=10,\n",
    "    start_q=0, max_q=10,\n",
    "    start_P=0, max_P=2,\n",
    "    start_Q=0, max_Q=2,\n",
    "    seasonal=False,           # set to True if you suspect seasonality; we'll keep False for daily data\n",
    "    test='adf',                # use ADF test to determine d\n",
    "    trace=True,\n",
    "    error_action='ignore',\n",
    "    suppress_warnings=True,\n",
    "    stepwise=True,\n",
    "    n_fits=50,\n",
    "    information_criterion='aic',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nBest ARIMA model:\", auto_model.order)\n",
    "print(\"AIC:\", auto_model.aic())\n",
    "print(auto_model.summary())\n",
    "\n",
    "# Save the auto_arima model (it contains the fitted ARIMA)\n",
    "model_path = os.path.join(\"models\", \"auto_arima_model.pkl\")\n",
    "joblib.dump(auto_model, model_path)\n",
    "logger.info(f\"Auto ARIMA model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630128e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 41: Walk-forward validation on validation set\n",
    "# We'll retrain the ARIMA model with the chosen order on an expanding window.\n",
    "# This simulates how the model would be used in practice.\n",
    "\n",
    "best_order = auto_model.order  # (p,d,q) from auto_arima\n",
    "\n",
    "def walk_forward_arima(train_series, test_series, order):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation for ARIMA.\n",
    "    For each day in test_series, refit the model on all available data up to that day,\n",
    "    then forecast one step ahead.\n",
    "    \"\"\"\n",
    "    history = list(train_series)\n",
    "    predictions = []\n",
    "    for t in range(len(test_series)):\n",
    "        model = ARIMA(history, order=order)\n",
    "        model_fit = model.fit()\n",
    "        forecast = model_fit.forecast(steps=1)[0]\n",
    "        predictions.append(forecast)\n",
    "        # Append the actual observation to history for next step\n",
    "        history.append(test_series.iloc[t])\n",
    "    return np.array(predictions)\n",
    "\n",
    "# First on validation set\n",
    "logger.info(\"Running walk-forward validation on validation set...\")\n",
    "val_pred = walk_forward_arima(train_close, val_close, best_order)\n",
    "\n",
    "# Evaluate\n",
    "val_actual = val_close.values\n",
    "val_rmse = np.sqrt(mean_squared_error(val_actual, val_pred))\n",
    "val_mae = mean_absolute_error(val_actual, val_pred)\n",
    "val_mape = np.mean(np.abs((val_actual - val_pred) / val_actual)) * 100\n",
    "\n",
    "# Directional accuracy\n",
    "val_direction_actual = (val_actual[1:] > val_actual[:-1]).astype(int)\n",
    "val_direction_pred = (val_pred[1:] > val_actual[:-1]).astype(int)  # compare predicted next to current actual\n",
    "val_dir_acc = np.mean(val_direction_pred == val_direction_actual) * 100\n",
    "\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(f\"RMSE: {val_rmse:.4f}\")\n",
    "print(f\"MAE: {val_mae:.4f}\")\n",
    "print(f\"MAPE: {val_mape:.2f}%\")\n",
    "print(f\"Directional Accuracy: {val_dir_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 42: Evaluate on test set using walk-forward validation (starting from combined train+val)\n",
    "logger.info(\"Running walk-forward validation on test set...\")\n",
    "# Combine train and validation for history before test\n",
    "combined_train_val = pd.concat([train_close, val_close])\n",
    "test_pred = walk_forward_arima(combined_train_val, test_close, best_order)\n",
    "\n",
    "test_actual = test_close.values\n",
    "test_rmse = np.sqrt(mean_squared_error(test_actual, test_pred))\n",
    "test_mae = mean_absolute_error(test_actual, test_pred)\n",
    "test_mape = np.mean(np.abs((test_actual - test_pred) / test_actual)) * 100\n",
    "\n",
    "test_direction_actual = (test_actual[1:] > test_actual[:-1]).astype(int)\n",
    "test_direction_pred = (test_pred[1:] > test_actual[:-1]).astype(int)\n",
    "test_dir_acc = np.mean(test_direction_pred == test_direction_actual) * 100\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"RMSE: {test_rmse:.4f}\")\n",
    "print(f\"MAE: {test_mae:.4f}\")\n",
    "print(f\"MAPE: {test_mape:.2f}%\")\n",
    "print(f\"Directional Accuracy: {test_dir_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 43: Plot validation and test predictions vs actual\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Validation\n",
    "axes[0].plot(val_close.index, val_actual, label='Actual', color='blue')\n",
    "axes[0].plot(val_close.index, val_pred, label='ARIMA Predicted', color='red', linestyle='--')\n",
    "axes[0].set_title(f'ARIMA Walk-Forward Validation – Validation Set (Order {best_order})')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Test\n",
    "axes[1].plot(test_close.index, test_actual, label='Actual', color='blue')\n",
    "axes[1].plot(test_close.index, test_pred, label='ARIMA Predicted', color='red', linestyle='--')\n",
    "axes[1].set_title(f'ARIMA Walk-Forward Validation – Test Set')\n",
    "axes[1].set_ylabel('Price ($)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"arima_predictions.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ad27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 44: Save ARIMA predictions for later comparison\n",
    "results_df = pd.DataFrame({\n",
    "    'date': test_close.index,\n",
    "    'actual': test_actual,\n",
    "    'arima_pred': test_pred,\n",
    "    'direction_actual': np.concatenate([test_direction_actual, [np.nan]]),  # align lengths\n",
    "    'direction_pred': np.concatenate([test_direction_pred, [np.nan]])\n",
    "})\n",
    "results_df.to_csv(os.path.join(PROCESSED_DATA_PATH, \"arima_test_results.csv\"), index=False)\n",
    "logger.info(\"ARIMA test results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 45: Diagnostics – Residual analysis\n",
    "# Fit ARIMA on full training+validation and check residuals\n",
    "final_model = ARIMA(combined_train_val, order=best_order)\n",
    "final_fit = final_model.fit()\n",
    "residuals = final_fit.resid\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Residuals over time\n",
    "axes[0,0].plot(residuals)\n",
    "axes[0,0].set_title('Residuals')\n",
    "axes[0,0].axhline(y=0, color='red', linestyle='--')\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[0,1].hist(residuals, bins=30, edgecolor='black')\n",
    "axes[0,1].set_title('Residuals Histogram')\n",
    "\n",
    "# ACF of residuals\n",
    "plot_acf(residuals, lags=40, ax=axes[1,0])\n",
    "axes[1,0].set_title('ACF of Residuals')\n",
    "\n",
    "# PACF of residuals\n",
    "plot_pacf(residuals, lags=40, ax=axes[1,1])\n",
    "axes[1,1].set_title('PACF of Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"arima_residuals.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Ljung-Box test for autocorrelation in residuals\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "lb_test = acorr_ljungbox(residuals, lags=[10,20,30], return_df=True)\n",
    "print(\"Ljung-Box test p-values:\")\n",
    "print(lb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 46: Summary of ARIMA baseline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ARIMA BASELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best order (p,d,q): {best_order}\")\n",
    "print(f\"AIC on training: {auto_model.aic():.2f}\")\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  RMSE: {val_rmse:.4f}   MAE: {val_mae:.4f}   MAPE: {val_mape:.2f}%   DirAcc: {val_dir_acc:.2f}%\")\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}   MAE: {test_mae:.4f}   MAPE: {test_mape:.2f}%   DirAcc: {test_dir_acc:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "logger.info(\"Phase 4 complete. ARIMA baseline established.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e201b65",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaba194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 47: Import additional libraries for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "\n",
    "logger.info(\"Starting Phase 5: LSTM Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 48: Load preprocessed sequences and targets\n",
    "PROCESSED_DATA_PATH = \"data/processed\"\n",
    "\n",
    "X_train_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_train_seq.npy\"))\n",
    "y_train_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_train_seq.npy\"))\n",
    "X_val_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_val_seq.npy\"))\n",
    "y_val_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_val_seq.npy\"))\n",
    "X_test_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_test_seq.npy\"))\n",
    "y_test_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_test_seq.npy\"))\n",
    "\n",
    "# Load dates if needed (optional)\n",
    "train_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"train_seq_dates.npy\"), allow_pickle=True)\n",
    "val_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"val_seq_dates.npy\"), allow_pickle=True)\n",
    "test_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"test_seq_dates.npy\"), allow_pickle=True)\n",
    "\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"X_val_seq shape: {X_val_seq.shape}\")\n",
    "print(f\"X_test_seq shape: {X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099684eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 49: Scale the target values for better training stability\n",
    "# We'll fit a scaler on the training targets only.\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Reshape to 2D for scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train_seq.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val_seq.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Save target scaler for inverse transformation later\n",
    "scaler_path = os.path.join(PROCESSED_DATA_PATH, \"target_scaler.pkl\")\n",
    "joblib.dump(target_scaler, scaler_path)\n",
    "print(f\"Target scaler saved to {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b26548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 50: Define LSTM model architecture\n",
    "def build_lstm_model(input_shape, lstm_units=[50, 50], dropout_rate=0.2, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    # First LSTM layer with return sequences for stacking\n",
    "    model.add(LSTM(units=lstm_units[0], return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(units=lstm_units[1], return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    model.add(Dense(1))  # Linear activation for regression\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])  # (window_size, n_features)\n",
    "lstm_units = [64, 32]  # can be tuned\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = build_lstm_model(input_shape, lstm_units, dropout_rate, learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 51: Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", \"lstm_best.keras\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "logger.info(\"Callbacks configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07732f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 52: Train the model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_scaled,\n",
    "    validation_data=(X_val_seq, y_val_scaled),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "with open(os.path.join(\"models\", \"lstm_history.pkl\"), 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "logger.info(\"LSTM training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 53: Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_title('Model MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"lstm_training_history.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 54: Load the best model (saved by ModelCheckpoint)\n",
    "best_model = load_model(os.path.join(\"models\", \"lstm_best.keras\"))\n",
    "logger.info(\"Best LSTM model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae506479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 55: Evaluate on validation and test sets\n",
    "# Validation set predictions\n",
    "y_val_pred_scaled = best_model.predict(X_val_seq, verbose=0)\n",
    "y_val_pred = target_scaler.inverse_transform(y_val_pred_scaled).flatten()\n",
    "y_val_actual = y_val_seq  # already original scale\n",
    "\n",
    "# Test set predictions\n",
    "y_test_pred_scaled = best_model.predict(X_test_seq, verbose=0)\n",
    "y_test_pred = target_scaler.inverse_transform(y_test_pred_scaled).flatten()\n",
    "y_test_actual = y_test_seq\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    mape = np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "    # Directional accuracy\n",
    "    direction_actual = (actual[1:] > actual[:-1]).astype(int)\n",
    "    direction_pred = (pred[1:] > actual[:-1]).astype(int)\n",
    "    dir_acc = np.mean(direction_pred == direction_actual) * 100\n",
    "    return rmse, mae, mape, dir_acc\n",
    "\n",
    "val_rmse, val_mae, val_mape, val_dir_acc = compute_metrics(y_val_actual, y_val_pred)\n",
    "test_rmse, test_mae, test_mape, test_dir_acc = compute_metrics(y_test_actual, y_test_pred)\n",
    "\n",
    "print(\"\\nLSTM Validation Set Performance:\")\n",
    "print(f\"RMSE: {val_rmse:.4f}   MAE: {val_mae:.4f}   MAPE: {val_mape:.2f}%   DirAcc: {val_dir_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nLSTM Test Set Performance:\")\n",
    "print(f\"RMSE: {test_rmse:.4f}   MAE: {test_mae:.4f}   MAPE: {test_mape:.2f}%   DirAcc: {test_dir_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 56: Plot predictions vs actual\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Validation\n",
    "axes[0].plot(val_seq_dates, y_val_actual, label='Actual', color='blue')\n",
    "axes[0].plot(val_seq_dates, y_val_pred, label='LSTM Predicted', color='red', linestyle='--')\n",
    "axes[0].set_title('LSTM Predictions – Validation Set')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Test\n",
    "axes[1].plot(test_seq_dates, y_test_actual, label='Actual', color='blue')\n",
    "axes[1].plot(test_seq_dates, y_test_pred, label='LSTM Predicted', color='red', linestyle='--')\n",
    "axes[1].set_title('LSTM Predictions – Test Set')\n",
    "axes[1].set_ylabel('Price ($)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"lstm_predictions.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffccefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 57: Compare with ARIMA baseline\n",
    "# Load ARIMA test results\n",
    "arima_results = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"arima_test_results.csv\"))\n",
    "# Note: ARIMA predictions are aligned with test set dates, but might have different length if walk-forward started earlier.\n",
    "# We'll align by date if needed. For simplicity, we'll use the metrics we already computed.\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<15} {'RMSE':<10} {'MAE':<10} {'MAPE(%)':<10} {'DirAcc(%)':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'ARIMA':<15} {test_rmse_arima:<10.4f} {test_mae_arima:<10.4f} {test_mape_arima:<10.2f} {test_dir_acc_arima:<10.2f}\")\n",
    "print(f\"{'LSTM':<15} {test_rmse:<10.4f} {test_mae:<10.4f} {test_mape:<10.2f} {test_dir_acc:<10.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# We need to retrieve ARIMA test metrics from earlier. Let's store them in a variable.\n",
    "# For completeness, recompute from saved results if needed.\n",
    "arima_test_rmse = np.sqrt(mean_squared_error(arima_results['actual'], arima_results['arima_pred']))\n",
    "arima_test_mae = mean_absolute_error(arima_results['actual'], arima_results['arima_pred'])\n",
    "arima_test_mape = np.mean(np.abs((arima_results['actual'] - arima_results['arima_pred']) / arima_results['actual'])) * 100\n",
    "arima_dir_acc = arima_results['direction_pred'].iloc[:-1].eq(arima_results['direction_actual'].iloc[:-1]).mean() * 100\n",
    "\n",
    "print(\"\\nUpdated comparison with recomputed ARIMA metrics:\")\n",
    "print(f\"ARIMA Test RMSE: {arima_test_rmse:.4f}, MAE: {arima_test_mae:.4f}, MAPE: {arima_test_mape:.2f}%, DirAcc: {arima_dir_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 58: Save LSTM predictions and metrics\n",
    "lstm_results = pd.DataFrame({\n",
    "    'date': test_seq_dates,\n",
    "    'actual': y_test_actual,\n",
    "    'lstm_pred': y_test_pred\n",
    "})\n",
    "lstm_results.to_csv(os.path.join(PROCESSED_DATA_PATH, \"lstm_test_results.csv\"), index=False)\n",
    "\n",
    "# Save metrics\n",
    "metrics_dict = {\n",
    "    'model': 'LSTM',\n",
    "    'val_rmse': val_rmse,\n",
    "    'val_mae': val_mae,\n",
    "    'val_mape': val_mape,\n",
    "    'val_dir_acc': val_dir_acc,\n",
    "    'test_rmse': test_rmse,\n",
    "    'test_mae': test_mae,\n",
    "    'test_mape': test_mape,\n",
    "    'test_dir_acc': test_dir_acc\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics_dict])\n",
    "metrics_df.to_csv(os.path.join(PROCESSED_DATA_PATH, \"lstm_metrics.csv\"), index=False)\n",
    "\n",
    "logger.info(\"LSTM results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb65b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 59: (Optional) Simple hyperparameter tuning\n",
    "# We could run a quick grid search on a few key parameters, but given time we'll skip for now.\n",
    "# If we wanted to, we'd use Keras Tuner or manual loops.\n",
    "logger.info(\"Phase 5 complete. LSTM model trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2203d1be",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b01e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 60: Import necessary libraries (if not already)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "logger.info(\"Starting Phase 6: GRU Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e839434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 61: Load preprocessed sequences and targets\n",
    "PROCESSED_DATA_PATH = \"data/processed\"\n",
    "\n",
    "X_train_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_train_seq.npy\"))\n",
    "y_train_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_train_seq.npy\"))\n",
    "X_val_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_val_seq.npy\"))\n",
    "y_val_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_val_seq.npy\"))\n",
    "X_test_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_test_seq.npy\"))\n",
    "y_test_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_test_seq.npy\"))\n",
    "\n",
    "train_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"train_seq_dates.npy\"), allow_pickle=True)\n",
    "val_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"val_seq_dates.npy\"), allow_pickle=True)\n",
    "test_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"test_seq_dates.npy\"), allow_pickle=True)\n",
    "\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"X_val_seq shape: {X_val_seq.shape}\")\n",
    "print(f\"X_test_seq shape: {X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed302ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 62: Load target scaler (fitted on training targets in Phase 5)\n",
    "target_scaler = joblib.load(os.path.join(PROCESSED_DATA_PATH, \"target_scaler.pkl\"))\n",
    "\n",
    "# Scale targets (use same scaler for consistency)\n",
    "y_train_scaled = target_scaler.transform(y_train_seq.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val_seq.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test_seq.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ad11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 63: Define GRU model architecture\n",
    "def build_gru_model(input_shape, gru_units=[64, 32], dropout_rate=0.3, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    # First GRU layer with return sequences for stacking\n",
    "    model.add(GRU(units=gru_units[0], return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second GRU layer\n",
    "    model.add(GRU(units=gru_units[1], return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    model.add(Dense(1))  # Linear activation for regression\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Model parameters (can be tuned)\n",
    "input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "gru_units = [64, 32]\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = build_gru_model(input_shape, gru_units, dropout_rate, learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 64: Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", \"gru_best.keras\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "logger.info(\"Callbacks configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 65: Train the GRU model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_scaled,\n",
    "    validation_data=(X_val_seq, y_val_scaled),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open(os.path.join(\"models\", \"gru_history.pkl\"), 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "logger.info(\"GRU training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 66: Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_title('GRU Model Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_title('GRU Model MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"gru_training_history.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb13218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 67: Load the best GRU model\n",
    "best_model = load_model(os.path.join(\"models\", \"gru_best.keras\"))\n",
    "logger.info(\"Best GRU model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 68: Evaluate on validation and test sets\n",
    "# Validation predictions\n",
    "y_val_pred_scaled = best_model.predict(X_val_seq, verbose=0)\n",
    "y_val_pred = target_scaler.inverse_transform(y_val_pred_scaled).flatten()\n",
    "y_val_actual = y_val_seq\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_scaled = best_model.predict(X_test_seq, verbose=0)\n",
    "y_test_pred = target_scaler.inverse_transform(y_test_pred_scaled).flatten()\n",
    "y_test_actual = y_test_seq\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    mape = np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "    direction_actual = (actual[1:] > actual[:-1]).astype(int)\n",
    "    direction_pred = (pred[1:] > actual[:-1]).astype(int)\n",
    "    dir_acc = np.mean(direction_pred == direction_actual) * 100\n",
    "    return rmse, mae, mape, dir_acc\n",
    "\n",
    "val_rmse, val_mae, val_mape, val_dir_acc = compute_metrics(y_val_actual, y_val_pred)\n",
    "test_rmse, test_mae, test_mape, test_dir_acc = compute_metrics(y_test_actual, y_test_pred)\n",
    "\n",
    "print(\"\\nGRU Validation Set Performance:\")\n",
    "print(f\"RMSE: {val_rmse:.4f}   MAE: {val_mae:.4f}   MAPE: {val_mape:.2f}%   DirAcc: {val_dir_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nGRU Test Set Performance:\")\n",
    "print(f\"RMSE: {test_rmse:.4f}   MAE: {test_mae:.4f}   MAPE: {test_mape:.2f}%   DirAcc: {test_dir_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1baef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 69: Plot GRU predictions vs actual\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Validation\n",
    "axes[0].plot(val_seq_dates, y_val_actual, label='Actual', color='blue')\n",
    "axes[0].plot(val_seq_dates, y_val_pred, label='GRU Predicted', color='red', linestyle='--')\n",
    "axes[0].set_title('GRU Predictions – Validation Set')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Test\n",
    "axes[1].plot(test_seq_dates, y_test_actual, label='Actual', color='blue')\n",
    "axes[1].plot(test_seq_dates, y_test_pred, label='GRU Predicted', color='red', linestyle='--')\n",
    "axes[1].set_title('GRU Predictions – Test Set')\n",
    "axes[1].set_ylabel('Price ($)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"gru_predictions.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 70: Compare with ARIMA and LSTM\n",
    "# Load ARIMA test results\n",
    "arima_results = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"arima_test_results.csv\"))\n",
    "arima_test_rmse = np.sqrt(mean_squared_error(arima_results['actual'], arima_results['arima_pred']))\n",
    "arima_test_mae = mean_absolute_error(arima_results['actual'], arima_results['arima_pred'])\n",
    "arima_test_mape = np.mean(np.abs((arima_results['actual'] - arima_results['arima_pred']) / arima_results['actual'])) * 100\n",
    "arima_dir_acc = arima_results['direction_pred'].iloc[:-1].eq(arima_results['direction_actual'].iloc[:-1]).mean() * 100\n",
    "\n",
    "# Load LSTM metrics\n",
    "lstm_metrics = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"lstm_metrics.csv\"))\n",
    "lstm_test_rmse = lstm_metrics['test_rmse'].values[0]\n",
    "lstm_test_mae = lstm_metrics['test_mae'].values[0]\n",
    "lstm_test_mape = lstm_metrics['test_mape'].values[0]\n",
    "lstm_test_dir_acc = lstm_metrics['test_dir_acc'].values[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<15} {'RMSE':<12} {'MAE':<12} {'MAPE(%)':<12} {'DirAcc(%)':<12}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'ARIMA':<15} {arima_test_rmse:<12.4f} {arima_test_mae:<12.4f} {arima_test_mape:<12.2f} {arima_dir_acc:<12.2f}\")\n",
    "print(f\"{'LSTM':<15} {lstm_test_rmse:<12.4f} {lstm_test_mae:<12.4f} {lstm_test_mape:<12.2f} {lstm_test_dir_acc:<12.2f}\")\n",
    "print(f\"{'GRU':<15} {test_rmse:<12.4f} {test_mae:<12.4f} {test_mape:<12.2f} {test_dir_acc:<12.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 71: Save GRU predictions and metrics\n",
    "gru_results = pd.DataFrame({\n",
    "    'date': test_seq_dates,\n",
    "    'actual': y_test_actual,\n",
    "    'gru_pred': y_test_pred\n",
    "})\n",
    "gru_results.to_csv(os.path.join(PROCESSED_DATA_PATH, \"gru_test_results.csv\"), index=False)\n",
    "\n",
    "# Save metrics\n",
    "gru_metrics_dict = {\n",
    "    'model': 'GRU',\n",
    "    'val_rmse': val_rmse,\n",
    "    'val_mae': val_mae,\n",
    "    'val_mape': val_mape,\n",
    "    'val_dir_acc': val_dir_acc,\n",
    "    'test_rmse': test_rmse,\n",
    "    'test_mae': test_mae,\n",
    "    'test_mape': test_mape,\n",
    "    'test_dir_acc': test_dir_acc\n",
    "}\n",
    "gru_metrics_df = pd.DataFrame([gru_metrics_dict])\n",
    "gru_metrics_df.to_csv(os.path.join(PROCESSED_DATA_PATH, \"gru_metrics.csv\"), index=False)\n",
    "\n",
    "logger.info(\"GRU results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a485d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 72: (Optional) Quick analysis of GRU vs LSTM\n",
    "improvement_rmse = (lstm_test_rmse - test_rmse) / lstm_test_rmse * 100\n",
    "improvement_dir = (test_dir_acc - lstm_test_dir_acc)\n",
    "\n",
    "print(\"\\nGRU vs LSTM on Test Set:\")\n",
    "print(f\"RMSE improvement: {improvement_rmse:+.2f}% (negative means GRU better)\")\n",
    "print(f\"Directional accuracy change: {improvement_dir:+.2f} percentage points\")\n",
    "\n",
    "logger.info(\"Phase 6 complete. GRU model trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c86f26",
   "metadata": {},
   "source": [
    "## CNN-BiGRU Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 73: Import necessary libraries (if not already)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Bidirectional, GRU, Dense, Dropout, Input, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "logger.info(\"Starting Phase 7: CNN-BiGRU Hybrid Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2942c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 74: Load preprocessed sequences and targets\n",
    "PROCESSED_DATA_PATH = \"data/processed\"\n",
    "\n",
    "X_train_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_train_seq.npy\"))\n",
    "y_train_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_train_seq.npy\"))\n",
    "X_val_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_val_seq.npy\"))\n",
    "y_val_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_val_seq.npy\"))\n",
    "X_test_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_test_seq.npy\"))\n",
    "y_test_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_test_seq.npy\"))\n",
    "\n",
    "train_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"train_seq_dates.npy\"), allow_pickle=True)\n",
    "val_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"val_seq_dates.npy\"), allow_pickle=True)\n",
    "test_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"test_seq_dates.npy\"), allow_pickle=True)\n",
    "\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"X_val_seq shape: {X_val_seq.shape}\")\n",
    "print(f\"X_test_seq shape: {X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb41c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 75: Load target scaler and scale targets\n",
    "target_scaler = joblib.load(os.path.join(PROCESSED_DATA_PATH, \"target_scaler.pkl\"))\n",
    "\n",
    "y_train_scaled = target_scaler.transform(y_train_seq.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val_seq.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test_seq.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 76: Define CNN-BiGRU model architecture\n",
    "def build_cnn_bigru_model(input_shape, \n",
    "                          conv_filters=64, \n",
    "                          kernel_size=3, \n",
    "                          pool_size=2, \n",
    "                          gru_units=50, \n",
    "                          dropout_rate=0.3, \n",
    "                          learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build a hybrid CNN-Bidirectional GRU model.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: (window_size, n_features)\n",
    "        conv_filters: number of filters in Conv1D\n",
    "        kernel_size: size of convolutional kernel\n",
    "        pool_size: pooling window size\n",
    "        gru_units: number of GRU units in each direction\n",
    "        dropout_rate: dropout rate after each layer\n",
    "        learning_rate: Adam learning rate\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    # 1D Convolutional layer for feature extraction\n",
    "    model.add(Conv1D(filters=conv_filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Bidirectional GRU layer\n",
    "    model.add(Bidirectional(GRU(units=gru_units, return_sequences=False)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    model.add(Dense(1))  # Linear activation for regression\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Model parameters (can be tuned)\n",
    "input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "conv_filters = 64\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "gru_units = 50\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = build_cnn_bigru_model(input_shape, conv_filters, kernel_size, pool_size, gru_units, dropout_rate, learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 77: Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", \"cnn_bigru_best.keras\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "logger.info(\"Callbacks configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 78: Train the CNN-BiGRU model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_scaled,\n",
    "    validation_data=(X_val_seq, y_val_scaled),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open(os.path.join(\"models\", \"cnn_bigru_history.pkl\"), 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "logger.info(\"CNN-BiGRU training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 79: Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_title('CNN-BiGRU Model Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_title('CNN-BiGRU Model MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"cnn_bigru_training_history.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 80: Load the best CNN-BiGRU model\n",
    "best_model = load_model(os.path.join(\"models\", \"cnn_bigru_best.keras\"))\n",
    "logger.info(\"Best CNN-BiGRU model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff727f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 81: Evaluate on validation and test sets\n",
    "# Validation predictions\n",
    "y_val_pred_scaled = best_model.predict(X_val_seq, verbose=0)\n",
    "y_val_pred = target_scaler.inverse_transform(y_val_pred_scaled).flatten()\n",
    "y_val_actual = y_val_seq\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_scaled = best_model.predict(X_test_seq, verbose=0)\n",
    "y_test_pred = target_scaler.inverse_transform(y_test_pred_scaled).flatten()\n",
    "y_test_actual = y_test_seq\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    mape = np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "    direction_actual = (actual[1:] > actual[:-1]).astype(int)\n",
    "    direction_pred = (pred[1:] > actual[:-1]).astype(int)\n",
    "    dir_acc = np.mean(direction_pred == direction_actual) * 100\n",
    "    return rmse, mae, mape, dir_acc\n",
    "\n",
    "val_rmse, val_mae, val_mape, val_dir_acc = compute_metrics(y_val_actual, y_val_pred)\n",
    "test_rmse, test_mae, test_mape, test_dir_acc = compute_metrics(y_test_actual, y_test_pred)\n",
    "\n",
    "print(\"\\nCNN-BiGRU Validation Set Performance:\")\n",
    "print(f\"RMSE: {val_rmse:.4f}   MAE: {val_mae:.4f}   MAPE: {val_mape:.2f}%   DirAcc: {val_dir_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nCNN-BiGRU Test Set Performance:\")\n",
    "print(f\"RMSE: {test_rmse:.4f}   MAE: {test_mae:.4f}   MAPE: {test_mape:.2f}%   DirAcc: {test_dir_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d866862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 82: Plot CNN-BiGRU predictions vs actual\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Validation\n",
    "axes[0].plot(val_seq_dates, y_val_actual, label='Actual', color='blue')\n",
    "axes[0].plot(val_seq_dates, y_val_pred, label='CNN-BiGRU Predicted', color='red', linestyle='--')\n",
    "axes[0].set_title('CNN-BiGRU Predictions – Validation Set')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Test\n",
    "axes[1].plot(test_seq_dates, y_test_actual, label='Actual', color='blue')\n",
    "axes[1].plot(test_seq_dates, y_test_pred, label='CNN-BiGRU Predicted', color='red', linestyle='--')\n",
    "axes[1].set_title('CNN-BiGRU Predictions – Test Set')\n",
    "axes[1].set_ylabel('Price ($)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"cnn_bigru_predictions.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 83: Compare with previous models\n",
    "# Load previous metrics\n",
    "arima_results = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"arima_test_results.csv\"))\n",
    "arima_test_rmse = np.sqrt(mean_squared_error(arima_results['actual'], arima_results['arima_pred']))\n",
    "arima_test_mae = mean_absolute_error(arima_results['actual'], arima_results['arima_pred'])\n",
    "arima_test_mape = np.mean(np.abs((arima_results['actual'] - arima_results['arima_pred']) / arima_results['actual'])) * 100\n",
    "arima_dir_acc = arima_results['direction_pred'].iloc[:-1].eq(arima_results['direction_actual'].iloc[:-1]).mean() * 100\n",
    "\n",
    "lstm_metrics = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"lstm_metrics.csv\"))\n",
    "lstm_test_rmse = lstm_metrics['test_rmse'].values[0]\n",
    "lstm_test_mae = lstm_metrics['test_mae'].values[0]\n",
    "lstm_test_mape = lstm_metrics['test_mape'].values[0]\n",
    "lstm_test_dir_acc = lstm_metrics['test_dir_acc'].values[0]\n",
    "\n",
    "gru_metrics = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"gru_metrics.csv\"))\n",
    "gru_test_rmse = gru_metrics['test_rmse'].values[0]\n",
    "gru_test_mae = gru_metrics['test_mae'].values[0]\n",
    "gru_test_mape = gru_metrics['test_mape'].values[0]\n",
    "gru_test_dir_acc = gru_metrics['test_dir_acc'].values[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"MODEL COMPARISON ON TEST SET\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Model':<18} {'RMSE':<12} {'MAE':<12} {'MAPE(%)':<12} {'DirAcc(%)':<12}\")\n",
    "print(\"-\"*90)\n",
    "print(f\"{'ARIMA':<18} {arima_test_rmse:<12.4f} {arima_test_mae:<12.4f} {arima_test_mape:<12.2f} {arima_dir_acc:<12.2f}\")\n",
    "print(f\"{'LSTM':<18} {lstm_test_rmse:<12.4f} {lstm_test_mae:<12.4f} {lstm_test_mape:<12.2f} {lstm_test_dir_acc:<12.2f}\")\n",
    "print(f\"{'GRU':<18} {gru_test_rmse:<12.4f} {gru_test_mae:<12.4f} {gru_test_mape:<12.2f} {gru_test_dir_acc:<12.2f}\")\n",
    "print(f\"{'CNN-BiGRU':<18} {test_rmse:<12.4f} {test_mae:<12.4f} {test_mape:<12.2f} {test_dir_acc:<12.2f}\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032bc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 84: Save CNN-BiGRU predictions and metrics\n",
    "cnn_bigru_results = pd.DataFrame({\n",
    "    'date': test_seq_dates,\n",
    "    'actual': y_test_actual,\n",
    "    'cnn_bigru_pred': y_test_pred\n",
    "})\n",
    "cnn_bigru_results.to_csv(os.path.join(PROCESSED_DATA_PATH, \"cnn_bigru_test_results.csv\"), index=False)\n",
    "\n",
    "# Save metrics\n",
    "cnn_bigru_metrics_dict = {\n",
    "    'model': 'CNN-BiGRU',\n",
    "    'val_rmse': val_rmse,\n",
    "    'val_mae': val_mae,\n",
    "    'val_mape': val_mape,\n",
    "    'val_dir_acc': val_dir_acc,\n",
    "    'test_rmse': test_rmse,\n",
    "    'test_mae': test_mae,\n",
    "    'test_mape': test_mape,\n",
    "    'test_dir_acc': test_dir_acc\n",
    "}\n",
    "cnn_bigru_metrics_df = pd.DataFrame([cnn_bigru_metrics_dict])\n",
    "cnn_bigru_metrics_df.to_csv(os.path.join(PROCESSED_DATA_PATH, \"cnn_bigru_metrics.csv\"), index=False)\n",
    "\n",
    "logger.info(\"CNN-BiGRU results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644dd83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 85: (Optional) Analyze feature importance via integrated gradients or permutation importance\n",
    "# This is a more advanced step, could be added later.\n",
    "\n",
    "logger.info(\"Phase 7 complete. CNN-BiGRU hybrid model trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465419d0",
   "metadata": {},
   "source": [
    "## Transformer for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767904d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 86: Import necessary libraries (if not already)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, LayerNormalization, \n",
    "                                     MultiHeadAttention, GlobalAveragePooling1D, \n",
    "                                     Add, Flatten)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "logger.info(\"Starting Phase 8: Transformer Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ab502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 87: Load preprocessed sequences and targets\n",
    "PROCESSED_DATA_PATH = \"data/processed\"\n",
    "\n",
    "X_train_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_train_seq.npy\"))\n",
    "y_train_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_train_seq.npy\"))\n",
    "X_val_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_val_seq.npy\"))\n",
    "y_val_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_val_seq.npy\"))\n",
    "X_test_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"X_test_seq.npy\"))\n",
    "y_test_seq = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_test_seq.npy\"))\n",
    "\n",
    "train_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"train_seq_dates.npy\"), allow_pickle=True)\n",
    "val_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"val_seq_dates.npy\"), allow_pickle=True)\n",
    "test_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"test_seq_dates.npy\"), allow_pickle=True)\n",
    "\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"X_val_seq shape: {X_val_seq.shape}\")\n",
    "print(f\"X_test_seq shape: {X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 88: Load target scaler and scale targets\n",
    "target_scaler = joblib.load(os.path.join(PROCESSED_DATA_PATH, \"target_scaler.pkl\"))\n",
    "\n",
    "y_train_scaled = target_scaler.transform(y_train_seq.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val_seq.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test_seq.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 89: Define Transformer Encoder Layer\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    \"\"\"\n",
    "    A single transformer encoder block.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input tensor\n",
    "        head_size: Dimension of each attention head\n",
    "        num_heads: Number of attention heads\n",
    "        ff_dim: Hidden layer size in feed-forward network\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    # Multi-head self-attention\n",
    "    attention = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
    "    attention = Dropout(dropout)(attention)\n",
    "    attention = LayerNormalization(epsilon=1e-6)(inputs + attention)  # Add & Norm\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(attention)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = Dense(inputs.shape[-1])(ff)  # Project back to original dimension\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    outputs = LayerNormalization(epsilon=1e-6)(attention + ff)  # Add & Norm\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def build_transformer_model(input_shape, head_size=64, num_heads=4, ff_dim=128, num_layers=2, dropout=0.1, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build a Transformer encoder for time series regression.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: (sequence_length, n_features)\n",
    "        head_size: Dimension of each attention head\n",
    "        num_heads: Number of attention heads\n",
    "        ff_dim: Hidden layer size in feed-forward network\n",
    "        num_layers: Number of transformer encoder blocks\n",
    "        dropout: Dropout rate\n",
    "        learning_rate: Adam learning rate\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Initial projection (optional) – could add a Dense layer to increase dimension\n",
    "    x = inputs\n",
    "    \n",
    "    # Stack transformer encoder blocks\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    # Global pooling to get a fixed-size output\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    # Final regression head\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout/2)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Model parameters (can be tuned)\n",
    "input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "head_size = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout, learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 90: Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models\", \"transformer_best.keras\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "logger.info(\"Callbacks configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 91: Train the Transformer model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_scaled,\n",
    "    validation_data=(X_val_seq, y_val_scaled),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open(os.path.join(\"models\", \"transformer_history.pkl\"), 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "logger.info(\"Transformer training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 92: Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_title('Transformer Model Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_title('Transformer Model MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"transformer_training_history.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3355ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 93: Load the best Transformer model\n",
    "best_model = load_model(os.path.join(\"models\", \"transformer_best.keras\"))\n",
    "logger.info(\"Best Transformer model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22829e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 94: Evaluate on validation and test sets\n",
    "# Validation predictions\n",
    "y_val_pred_scaled = best_model.predict(X_val_seq, verbose=0)\n",
    "y_val_pred = target_scaler.inverse_transform(y_val_pred_scaled).flatten()\n",
    "y_val_actual = y_val_seq\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_scaled = best_model.predict(X_test_seq, verbose=0)\n",
    "y_test_pred = target_scaler.inverse_transform(y_test_pred_scaled).flatten()\n",
    "y_test_actual = y_test_seq\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    mape = np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "    direction_actual = (actual[1:] > actual[:-1]).astype(int)\n",
    "    direction_pred = (pred[1:] > actual[:-1]).astype(int)\n",
    "    dir_acc = np.mean(direction_pred == direction_actual) * 100\n",
    "    return rmse, mae, mape, dir_acc\n",
    "\n",
    "val_rmse, val_mae, val_mape, val_dir_acc = compute_metrics(y_val_actual, y_val_pred)\n",
    "test_rmse, test_mae, test_mape, test_dir_acc = compute_metrics(y_test_actual, y_test_pred)\n",
    "\n",
    "print(\"\\nTransformer Validation Set Performance:\")\n",
    "print(f\"RMSE: {val_rmse:.4f}   MAE: {val_mae:.4f}   MAPE: {val_mape:.2f}%   DirAcc: {val_dir_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nTransformer Test Set Performance:\")\n",
    "print(f\"RMSE: {test_rmse:.4f}   MAE: {test_mae:.4f}   MAPE: {test_mape:.2f}%   DirAcc: {test_dir_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c541dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 95: Plot Transformer predictions vs actual\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Validation\n",
    "axes[0].plot(val_seq_dates, y_val_actual, label='Actual', color='blue')\n",
    "axes[0].plot(val_seq_dates, y_val_pred, label='Transformer Predicted', color='red', linestyle='--')\n",
    "axes[0].set_title('Transformer Predictions – Validation Set')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Test\n",
    "axes[1].plot(test_seq_dates, y_test_actual, label='Actual', color='blue')\n",
    "axes[1].plot(test_seq_dates, y_test_pred, label='Transformer Predicted', color='red', linestyle='--')\n",
    "axes[1].set_title('Transformer Predictions – Test Set')\n",
    "axes[1].set_ylabel('Price ($)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"transformer_predictions.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 96: Compare with previous models\n",
    "# Load ARIMA results\n",
    "arima_results = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"arima_test_results.csv\"))\n",
    "arima_test_rmse = np.sqrt(mean_squared_error(arima_results['actual'], arima_results['arima_pred']))\n",
    "arima_test_mae = mean_absolute_error(arima_results['actual'], arima_results['arima_pred'])\n",
    "arima_test_mape = np.mean(np.abs((arima_results['actual'] - arima_results['arima_pred']) / arima_results['actual'])) * 100\n",
    "arima_dir_acc = arima_results['direction_pred'].iloc[:-1].eq(arima_results['direction_actual'].iloc[:-1]).mean() * 100\n",
    "\n",
    "# Load other metrics\n",
    "lstm_metrics = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"lstm_metrics.csv\"))\n",
    "gru_metrics = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"gru_metrics.csv\"))\n",
    "cnn_bigru_metrics = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"cnn_bigru_metrics.csv\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL MODEL COMPARISON ON TEST SET\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Model':<20} {'RMSE':<12} {'MAE':<12} {'MAPE(%)':<12} {'DirAcc(%)':<12}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'ARIMA':<20} {arima_test_rmse:<12.4f} {arima_test_mae:<12.4f} {arima_test_mape:<12.2f} {arima_dir_acc:<12.2f}\")\n",
    "print(f\"{'LSTM':<20} {lstm_metrics['test_rmse'].values[0]:<12.4f} {lstm_metrics['test_mae'].values[0]:<12.4f} {lstm_metrics['test_mape'].values[0]:<12.2f} {lstm_metrics['test_dir_acc'].values[0]:<12.2f}\")\n",
    "print(f\"{'GRU':<20} {gru_metrics['test_rmse'].values[0]:<12.4f} {gru_metrics['test_mae'].values[0]:<12.4f} {gru_metrics['test_mape'].values[0]:<12.2f} {gru_metrics['test_dir_acc'].values[0]:<12.2f}\")\n",
    "print(f\"{'CNN-BiGRU':<20} {cnn_bigru_metrics['test_rmse'].values[0]:<12.4f} {cnn_bigru_metrics['test_mae'].values[0]:<12.4f} {cnn_bigru_metrics['test_mape'].values[0]:<12.2f} {cnn_bigru_metrics['test_dir_acc'].values[0]:<12.2f}\")\n",
    "print(f\"{'Transformer':<20} {test_rmse:<12.4f} {test_mae:<12.4f} {test_mape:<12.2f} {test_dir_acc:<12.2f}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf75a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 97: Save Transformer predictions and metrics\n",
    "transformer_results = pd.DataFrame({\n",
    "    'date': test_seq_dates,\n",
    "    'actual': y_test_actual,\n",
    "    'transformer_pred': y_test_pred\n",
    "})\n",
    "transformer_results.to_csv(os.path.join(PROCESSED_DATA_PATH, \"transformer_test_results.csv\"), index=False)\n",
    "\n",
    "# Save metrics\n",
    "transformer_metrics_dict = {\n",
    "    'model': 'Transformer',\n",
    "    'val_rmse': val_rmse,\n",
    "    'val_mae': val_mae,\n",
    "    'val_mape': val_mape,\n",
    "    'val_dir_acc': val_dir_acc,\n",
    "    'test_rmse': test_rmse,\n",
    "    'test_mae': test_mae,\n",
    "    'test_mape': test_mape,\n",
    "    'test_dir_acc': test_dir_acc\n",
    "}\n",
    "transformer_metrics_df = pd.DataFrame([transformer_metrics_dict])\n",
    "transformer_metrics_df.to_csv(os.path.join(PROCESSED_DATA_PATH, \"transformer_metrics.csv\"), index=False)\n",
    "\n",
    "logger.info(\"Transformer results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a57cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 98: (Optional) Visualize attention weights for a sample\n",
    "# This is an advanced diagnostic; can be added later.\n",
    "\n",
    "logger.info(\"Phase 8 complete. Transformer model trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf2288",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b02bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 99: Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "\n",
    "logger.info(\"Starting Phase 9: Ensemble Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd11921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 100: Load test dates and actual values\n",
    "PROCESSED_DATA_PATH = \"data/processed\"\n",
    "test_seq_dates = np.load(os.path.join(PROCESSED_DATA_PATH, \"test_seq_dates.npy\"), allow_pickle=True)\n",
    "y_test_actual = np.load(os.path.join(PROCESSED_DATA_PATH, \"y_test_seq.npy\"))\n",
    "\n",
    "# Load predictions from each model (assuming saved CSV files exist)\n",
    "# If not, you can regenerate them from the saved models, but we'll use the saved CSV for simplicity.\n",
    "\n",
    "models = ['lstm', 'gru', 'cnn_bigru', 'transformer']\n",
    "predictions = {}\n",
    "for model_name in models:\n",
    "    file_path = os.path.join(PROCESSED_DATA_PATH, f\"{model_name}_test_results.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Ensure alignment by date (if needed, but we trust they are in same order)\n",
    "        predictions[model_name] = df[f\"{model_name}_pred\"].values\n",
    "        logger.info(f\"Loaded {model_name} predictions, shape: {predictions[model_name].shape}\")\n",
    "    else:\n",
    "        logger.warning(f\"Predictions file for {model_name} not found. Skipping.\")\n",
    "\n",
    "# Also load ARIMA for comparison (optional)\n",
    "arima_path = os.path.join(PROCESSED_DATA_PATH, \"arima_test_results.csv\")\n",
    "if os.path.exists(arima_path):\n",
    "    arima_df = pd.read_csv(arima_path)\n",
    "    # ARIMA predictions might be shorter if walk-forward started later; we need to align.\n",
    "    # For simplicity, we'll keep only models with full length.\n",
    "    # Check lengths\n",
    "    for name, pred in predictions.items():\n",
    "        print(f\"{name}: {len(pred)}\")\n",
    "    print(f\"Actual: {len(y_test_actual)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 101: Ensure all predictions have the same length\n",
    "# Some models might have slightly different lengths due to sequence creation offsets.\n",
    "# We'll truncate to the minimum common length.\n",
    "lengths = [len(y_test_actual)] + [len(pred) for pred in predictions.values()]\n",
    "min_len = min(lengths)\n",
    "if min_len < len(y_test_actual):\n",
    "    logger.info(f\"Truncating all arrays to minimum length: {min_len}\")\n",
    "    y_test_actual = y_test_actual[:min_len]\n",
    "    for name in predictions:\n",
    "        predictions[name] = predictions[name][:min_len]\n",
    "    test_seq_dates = test_seq_dates[:min_len]\n",
    "\n",
    "print(f\"Final shapes: actual {y_test_actual.shape}, predictions all {len(predictions['lstm'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 102: Simple average ensemble\n",
    "# Stack predictions into a 2D array (n_samples, n_models)\n",
    "pred_array = np.column_stack([predictions[name] for name in models])\n",
    "ensemble_simple_avg = np.mean(pred_array, axis=1)\n",
    "\n",
    "# Compute metrics for simple average\n",
    "def compute_metrics(actual, pred, name=\"Ensemble\"):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    mape = np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "    direction_actual = (actual[1:] > actual[:-1]).astype(int)\n",
    "    direction_pred = (pred[1:] > actual[:-1]).astype(int)\n",
    "    dir_acc = np.mean(direction_pred == direction_actual) * 100\n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"RMSE: {rmse:.4f}   MAE: {mae:.4f}   MAPE: {mape:.2f}%   DirAcc: {dir_acc:.2f}%\")\n",
    "    return rmse, mae, mape, dir_acc\n",
    "\n",
    "simple_metrics = compute_metrics(y_test_actual, ensemble_simple_avg, \"Simple Average Ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ff8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 103: Weighted average ensemble based on validation performance\n",
    "# Load validation metrics for each model\n",
    "val_metrics = {}\n",
    "for model_name in models:\n",
    "    metrics_file = os.path.join(PROCESSED_DATA_PATH, f\"{model_name}_metrics.csv\")\n",
    "    if os.path.exists(metrics_file):\n",
    "        df = pd.read_csv(metrics_file)\n",
    "        # Use validation RMSE (lower is better) to compute weights\n",
    "        # We'll use inverse RMSE as weight\n",
    "        val_rmse = df['val_rmse'].values[0]\n",
    "        val_metrics[model_name] = val_rmse\n",
    "    else:\n",
    "        logger.warning(f\"Metrics file for {model_name} not found. Using equal weight.\")\n",
    "\n",
    "if val_metrics:\n",
    "    # Compute weights: inverse of RMSE, normalized\n",
    "    inv_rmse = {name: 1/rmse for name, rmse in val_metrics.items()}\n",
    "    total = sum(inv_rmse.values())\n",
    "    weights = {name: inv_rmse[name]/total for name in inv_rmse}\n",
    "    print(\"Weights based on validation RMSE:\")\n",
    "    for name, w in weights.items():\n",
    "        print(f\"  {name}: {w:.4f}\")\n",
    "    \n",
    "    # Apply weighted average\n",
    "    weighted_pred = np.zeros_like(ensemble_simple_avg)\n",
    "    for i, name in enumerate(models):\n",
    "        if name in weights:\n",
    "            weighted_pred += weights[name] * predictions[name]\n",
    "    \n",
    "    weighted_metrics = compute_metrics(y_test_actual, weighted_pred, \"Weighted Average Ensemble\")\n",
    "else:\n",
    "    logger.warning(\"No validation metrics found, skipping weighted ensemble.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 104: Compare ensemble with individual models\n",
    "# Load individual model test metrics\n",
    "individual_metrics = {}\n",
    "for model_name in models:\n",
    "    metrics_file = os.path.join(PROCESSED_DATA_PATH, f\"{model_name}_metrics.csv\")\n",
    "    if os.path.exists(metrics_file):\n",
    "        df = pd.read_csv(metrics_file)\n",
    "        individual_metrics[model_name] = {\n",
    "            'RMSE': df['test_rmse'].values[0],\n",
    "            'MAE': df['test_mae'].values[0],\n",
    "            'MAPE': df['test_mape'].values[0],\n",
    "            'DirAcc': df['test_dir_acc'].values[0]\n",
    "        }\n",
    "\n",
    "# Add ARIMA if available\n",
    "if os.path.exists(arima_path):\n",
    "    individual_metrics['ARIMA'] = {\n",
    "        'RMSE': arima_test_rmse,\n",
    "        'MAE': arima_test_mae,\n",
    "        'MAPE': arima_test_mape,\n",
    "        'DirAcc': arima_dir_acc\n",
    "    }\n",
    "\n",
    "# Add ensemble metrics\n",
    "individual_metrics['Simple_Avg_Ensemble'] = {\n",
    "    'RMSE': simple_metrics[0],\n",
    "    'MAE': simple_metrics[1],\n",
    "    'MAPE': simple_metrics[2],\n",
    "    'DirAcc': simple_metrics[3]\n",
    "}\n",
    "if 'weighted_metrics' in locals():\n",
    "    individual_metrics['Weighted_Ensemble'] = {\n",
    "        'RMSE': weighted_metrics[0],\n",
    "        'MAE': weighted_metrics[1],\n",
    "        'MAPE': weighted_metrics[2],\n",
    "        'DirAcc': weighted_metrics[3]\n",
    "    }\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame(individual_metrics).T\n",
    "comparison_df = comparison_df.round(4)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL ENSEMBLE COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string())\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(os.path.join(PROCESSED_DATA_PATH, \"ensemble_comparison.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 105: Plot ensemble predictions vs actual\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(test_seq_dates, y_test_actual, label='Actual', color='blue', linewidth=1.5)\n",
    "plt.plot(test_seq_dates, ensemble_simple_avg, label='Simple Average Ensemble', color='red', linestyle='--', linewidth=1.5)\n",
    "if 'weighted_pred' in locals():\n",
    "    plt.plot(test_seq_dates, weighted_pred, label='Weighted Ensemble', color='green', linestyle=':', linewidth=1.5)\n",
    "plt.title('Ensemble Predictions vs Actual - Test Set')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PROCESSED_DATA_PATH, \"ensemble_predictions.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 106: Save ensemble predictions\n",
    "ensemble_results = pd.DataFrame({\n",
    "    'date': test_seq_dates,\n",
    "    'actual': y_test_actual,\n",
    "    'simple_avg_ensemble': ensemble_simple_avg\n",
    "})\n",
    "if 'weighted_pred' in locals():\n",
    "    ensemble_results['weighted_ensemble'] = weighted_pred\n",
    "ensemble_results.to_csv(os.path.join(PROCESSED_DATA_PATH, \"ensemble_test_results.csv\"), index=False)\n",
    "\n",
    "logger.info(\"Phase 9 complete. Ensemble model evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843b961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
